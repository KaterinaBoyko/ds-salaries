from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error

# Выберем целевую переменную и признаки для моделей
X = data_cleaned[['work_year', 'remote_ratio']]
y = data_cleaned['salary_in_usd']

# Разделим данные на обучающую и тестовую выборки
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Построение модели Random Forest
rf_model = RandomForestRegressor(random_state=42)
rf_model.fit(X_train, y_train)
rf_predictions = rf_model.predict(X_test)

# Построение модели Gradient Boosting
gb_model = GradientBoostingRegressor(random_state=42)
gb_model.fit(X_train, y_train)
gb_predictions = gb_model.predict(X_test)

# Оценим качество моделей
rf_mae = mean_absolute_error(y_test, rf_predictions)
rf_mse = mean_squared_error(y_test, rf_predictions)
rf_r2 = r2_score(y_test, rf_predictions)

gb_mae = mean_absolute_error(y_test, gb_predictions)
gb_mse = mean_squared_error(y_test, gb_predictions)
gb_r2 = r2_score(y_test, gb_predictions)

# Оценим важность признаков в Random Forest
rf_feature_importances = rf_model.feature_importances_

# Оценим важность признаков в Gradient Boosting
gb_feature_importances = gb_model.feature_importances_

rf_mae, rf_mse, rf_r2, gb_mae, gb_mse, gb_r2, rf_feature_importances, gb_feature_importances
